{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Street View House Numbers - Indexed Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import svhn\n",
    "import graphics\n",
    "import keras_utils\n",
    "from keras.utils import np_utils\n",
    "import preprocessing\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# define some constants\n",
    "max_digits = 7\n",
    "image_size = (54,128)\n",
    "checkpoint_path = '../checkpoints/model.hdf5'\n",
    "resume_training = True\n",
    "\n",
    "# print the keras version used\n",
    "import keras\n",
    "print \"Keras version : {}\".format(keras.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate data\n",
    "For training, we will use a flattened dataset, where each training sample represents one digit. The inputs per sample will be resized image and the index of digit which needs to be detected, and outputs will be the number of digits in the image and the digit at given index.\n",
    "\n",
    "Because of flattening the dataset, the training set for label detector will be expanded, while the one for counter will have repeated samples. So in a way, the label detector will have more data to train with, but the counter can be assumed to be trained for more epochs.\n",
    "\n",
    "change **nsamples** below to **33402** to train with full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# read the h5py data file (takes time)\n",
    "rawdata = svhn.read_process_h5('../inputs/train/digitStruct.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nsamples = 100\n",
    "\n",
    "def generateData(data, n=1000):\n",
    "    '''\n",
    "    generates flattened SVHN dataset\n",
    "    '''\n",
    "    Ximg_flat = []\n",
    "    Xidx_flat = []\n",
    "    ycount_flat = []\n",
    "    ycoord_flat = []\n",
    "    ylabel_flat = []\n",
    "    \n",
    "    for datapoint in np.random.choice(data, size=n, replace=False):\n",
    "        img,_ = svhn.createImageData(datapoint, image_size, '../inputs/train/')\n",
    "        for i in range(0,datapoint['length']):\n",
    "            Ximg_flat.append(img)\n",
    "            Xidx_flat.append(i)\n",
    "            ycount_flat.append(datapoint['length'])\n",
    "            ylabel_flat.append(datapoint['labels'][i])\n",
    "            \n",
    "    ylabel_flat = [0 if y==10 else int(y) for y in ylabel_flat]\n",
    "    return np.array(Ximg_flat), np.array(Xidx_flat), np.array(ycount_flat), np.array(ylabel_flat)\n",
    "\n",
    "Ximg, Xidx, ycount, ylabel = generateData(rawdata, nsamples)\n",
    "Xidx = np_utils.to_categorical(Xidx, max_digits)\n",
    "ycount = np_utils.to_categorical(ycount, max_digits)\n",
    "ylabel = np_utils.to_categorical(ylabel, 10)\n",
    "\n",
    "Ximg, Ximg_val, Xidx, Xidx_val, ycount, ycount_val, ylabel, ylabel_val = train_test_split(Ximg, Xidx, ycount, ylabel, test_size=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image augmentation and normalization\n",
    "We will use a custom version of Keras' ImageDataGenerator. The default implementation only supports single input and single output vectors. The custom SVHNImageDataGenerator returns batches of multiple inputs and multiple outputs.\n",
    "ImageDataGenerator however has an open bug https://github.com/fchollet/keras/issues/2559 related to sample based normalization. The custom version has a fix for it.\n",
    "\n",
    "Also, since SVHNImageDataGenerator normalizes augmented training samples, we need to normalize validation data also."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def standardize(img):\n",
    "    s = img - np.mean(img, axis=(2,0,1), keepdims=True)\n",
    "    s /= np.std(s, axis=(2,0,1), keepdims=True)\n",
    "    return s\n",
    "Xs_val = np.array([standardize(x) for x in Ximg_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# use custom version of keras image augmentation system\n",
    "# also normalize the input image\n",
    "datagen = preprocessing.SVHNImageDataGenerator(\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.15,\n",
    "    shear_range=0.4,\n",
    "    zoom_range=[1.0,1.6],\n",
    "    samplewise_center=True,\n",
    "    samplewise_std_normalization=True\n",
    "    )\n",
    "\n",
    "datagen.fit(Ximg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Deep Learning Network\n",
    "Our deep learning network consists of a shared vision model, a counter and a label detector.\n",
    "\n",
    "The vision model processes input image of fixed size using Convolutional Neural Networks and produces a dense tensor of shape (1024,). This tensor is then processed by a counter which is made of fully connected layers. The counter output is used to generates indices, which are combined with the vision model output and fed to label detector, which outputs a label for each index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, Merge, Flatten, Dropout, merge\n",
    "from keras.layers.convolutional import Convolution2D\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "# define vision model\n",
    "image_in_vision = Input(shape=(image_size[0],image_size[1],3))\n",
    "x = BatchNormalization(axis=3)(image_in_vision)\n",
    "x = Convolution2D(32, 3, 3, activation='tanh')(x)\n",
    "x = BatchNormalization(axis=3)(x)\n",
    "x = Convolution2D(32, 3, 3, activation='relu')(x)\n",
    "x = BatchNormalization(axis=3)(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = MaxPooling2D(pool_size=(2,2))(x)\n",
    "x = Convolution2D(64, 3, 3, activation='relu')(x)\n",
    "x = BatchNormalization(axis=3)(x)\n",
    "x = Convolution2D(64, 3, 3, activation='relu')(x)\n",
    "x = BatchNormalization(axis=3)(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = MaxPooling2D(pool_size=(2,2))(x)\n",
    "x = Convolution2D(128, 3, 3, activation='relu')(x)\n",
    "x = BatchNormalization(axis=3)(x)\n",
    "x = Convolution2D(128, 3, 3, activation='relu')(x)\n",
    "x = BatchNormalization(axis=3)(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = MaxPooling2D(pool_size=(2,2))(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "h = BatchNormalization()(x)\n",
    "vision_model = Model(input=image_in_vision, output=h, name='vision')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define counter model\n",
    "h_in_counter = Input(shape=(1024,))\n",
    "yc = Dense(256, activation='relu')(h_in_counter)\n",
    "yc = BatchNormalization()(yc)\n",
    "yc = Dropout(0.2)(yc)\n",
    "yc = Dense(max_digits, activation='softmax')(yc)\n",
    "counter_model = Model(input=h_in_counter, output=yc, name='counter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define detector model\n",
    "h_in_detector = Input(shape=(1024,))\n",
    "idx_in_detector = Input(shape=(max_digits,))\n",
    "yl = merge([h_in_detector, idx_in_detector], mode='concat') \n",
    "yl = Dense(512, activation='relu')(yl)\n",
    "yl = BatchNormalization()(yl)\n",
    "yl = Dense(512, activation='relu')(yl)\n",
    "yl = BatchNormalization()(yl)\n",
    "yl = Dropout(0.2)(yl)\n",
    "yl = Dense(10, activation='softmax')(yl)\n",
    "\n",
    "detector_model = Model(input=[h_in_detector, idx_in_detector], output=yl, name='detector')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Combine the three models to construct training graph\n",
    "# We're defining the training graph as a composite made of macro models. \n",
    "# This will enable us to easily retrieve these macro components and restructure them during inference.\n",
    "Ximg_in = Input(shape=(image_size[0], image_size[1], 3), name='train_input_img')\n",
    "Xidx_in = Input(shape=(max_digits,), name='train_input_idx')\n",
    "h = vision_model(Ximg_in)\n",
    "yc = counter_model(h)\n",
    "yl = detector_model([h, Xidx_in])\n",
    "\n",
    "train_graph = Model(input=[Ximg_in, Xidx_in], output=[yc, yl])\n",
    "train_graph.compile(optimizer='adamax', loss=['categorical_crossentropy','categorical_crossentropy'], metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save the entire training graph\n",
    "with open('../checkpoints/model.yaml','w') as model_def:\n",
    "    model_def.write(train_graph.to_yaml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# resume training if instructed and weights file exists\n",
    "import os.path\n",
    "if resume_training and os.path.isfile(checkpoint_path):\n",
    "    model.load_weights(checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "DynamicPlot callback displays dynamically updated training metric plots instead of default progress bars. This gives a concise but dynamic view of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define checkpoint callback\n",
    "checkpoint = keras.callbacks.ModelCheckpoint(filepath=checkpoint_path, monitor='val_loss', mode='min', save_best_only=True)\n",
    "earlystop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, mode='min')\n",
    "\n",
    "history = train_graph.fit_generator(datagen.flow(Ximg, Xidx, ycount, ylabel, batch_size=64),\n",
    "                          nb_epoch=2, samples_per_epoch=len(Xidx),\n",
    "                          validation_data=([Xs_val, Xidx_val],[ycount_val, ylabel_val]),\n",
    "                          callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training History\n",
    "Here we plot the loss and accuracy metrics of counter and detector from Keras history callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,4))\n",
    "plt.subplot(1, 2, 1)\n",
    "ctl_plot = plt.plot(history.history['counter_loss'], 'r', label='training')\n",
    "cvl_plot = plt.plot(history.history['val_counter_loss'], 'g', label='validation')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.title('counter loss')\n",
    "plt.legend()\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['counter_acc'], 'r', label='training')\n",
    "plt.plot(history.history['val_counter_acc'], 'g', label='validation')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.title('counter accuracy')\n",
    "plt.legend(loc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['detector_loss'], 'r', label='training')\n",
    "plt.plot(history.history['val_detector_loss'], 'g', label='validation')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.title('detector loss')\n",
    "plt.legend()\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['detector_acc'], 'r', label='training')\n",
    "plt.plot(history.history['val_detector_acc'], 'g', label='validation')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.title('detector accuracy')\n",
    "plt.legend(loc=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [tensorflow]",
   "language": "python",
   "name": "Python [tensorflow]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
